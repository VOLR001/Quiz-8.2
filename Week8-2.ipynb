{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8.2: Train Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load image data using `torchvision.datasets.ImageFolder()` to train a network in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, optim\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import memory_profiler  # conda install -c anaconda memory_profiler  |  conda install -c conda-forge memory_profiler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from utils.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datasets, Dataloaders, and Transforms\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch` and `torchvision` provide out-of-the-box functionality for loading in lots of different kinds of data. The way you create a dataloader depends on the data you have (i.e., do you have numpy arrays, tensors, images, or something else?) and the PyTorch docs [can help you out](https://pytorch.org/docs/stable/data.html#dataset-types)\n",
    "\n",
    "Loading data into PyTorch is usually a two-step process:\n",
    "1. Create a `dataset` (this is your raw data)\n",
    "2. Create a `dataloader` (this will help you batch your data)\n",
    "\n",
    "Working with CNNs and images, you'll mostly be using `torchvision.datasets.ImageFolder()` ([docs](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)), it's very easy to use. It assumes you have a directory structure with sub-directories for each class like this:\n",
    "\n",
    "```\n",
    "data\n",
    "│\n",
    "├── class_1\n",
    "│   ├── image_1.png \n",
    "│   ├── image_2.png\n",
    "│   ├── image_3.png\n",
    "│   └── etc.\n",
    "└── class_2\n",
    "    ├── image_1.png \n",
    "    ├── image_2.png\n",
    "    ├── image_3.png\n",
    "    └── etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the training dataset we have in the current directory at `data/bitmoji_rgb`:\n",
    "\n",
    "```\n",
    "bitmoji_rgb\n",
    "└── train\n",
    "    ├── not_tom\n",
    "    │   ├── image_1.png \n",
    "    │   ├── image_2.png\n",
    "    │   ├── image_3.png\n",
    "    │   └── etc.\n",
    "    └── tom\n",
    "        ├── image_1.png \n",
    "        ├── image_2.png\n",
    "        ├── image_3.png\n",
    "        └── etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"data/bitmoji_rgb/train/\"\n",
    "\n",
    "mem = memory_profiler.memory_usage()[0]\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR)\n",
    "print(f\"Memory consumed: {memory_profiler.memory_usage()[0] - mem:.0f} mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how our memory usage is the same, we aren't loading anything in yet, just making PyTorch aware of what kind of data we have and where it is. We can now check various information about our `train_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Class count: {train_dataset.targets.count(0)}, {train_dataset.targets.count(1)}\")\n",
    "print(f\"Samples:\",len(train_dataset))\n",
    "print(f\"First sample: {train_dataset.samples[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could start working with this dataset directly. For example, here's the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = next(iter(train_dataset))\n",
    "print(f\"Class: {train_dataset.classes[target]}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But often we want to apply some pre-processing to our data. For example, `ImageFolder` loads our data using the `PIL` package, but we need tensors! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image data type: {type(img)}\")\n",
    "print(f\"     Image size: {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any pre-processing we wish to apply to our images is done using `torchvision.transforms`. There are a lot of transformation options here - we'll explore some more later - for now, we'll `Resize()` our images and convert them `ToTensor()`. We use `transforms.Compose()` to chain multiple transformations together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR,\n",
    "                                                 transform=data_transforms)\n",
    "img, target = next(iter(train_dataset))\n",
    "print(f\"Image data type: {type(img)}\")\n",
    "print(f\"     Image size: {img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay cool, but there's one more issue: we want to work with **batches** of data, because most of the time, we won't be able to fit an entire dataset into RAM at once (especially when it comes to image data). This is where PyTorch's `dataloader` comes in. It allows us to specify how we want to batch our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory_profiler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m----> 3\u001b[0m mem \u001b[38;5;241m=\u001b[39m memory_profiler\u001b[38;5;241m.\u001b[39mmemory_usage()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset,          \u001b[38;5;66;03m# our raw data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m                                            batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,  \u001b[38;5;66;03m# the size of batches we want the dataloader to return\u001b[39;00m\n\u001b[0;32m      6\u001b[0m                                            shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,           \u001b[38;5;66;03m# shuffle our data before batching\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                                            drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)        \u001b[38;5;66;03m# don't drop the last batch even if it's smaller than batch_size\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory consumed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_profiler\u001b[38;5;241m.\u001b[39mmemory_usage()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mmem\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory_profiler' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "mem = memory_profiler.memory_usage()[0]\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,          # our raw data\n",
    "                                           batch_size=BATCH_SIZE,  # the size of batches we want the dataloader to return\n",
    "                                           shuffle=True,           # shuffle our data before batching\n",
    "                                           drop_last=False)        # don't drop the last batch even if it's smaller than batch_size\n",
    "print(f\"Memory consumed: {memory_profiler.memory_usage()[0] - mem:.0f} mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we aren't loading anything yet, we just prepared the loader. We can now query the loader to return a batch of data (this will consume memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'memory_profiler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mem \u001b[38;5;241m=\u001b[39m memory_profiler\u001b[38;5;241m.\u001b[39mmemory_usage()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m imgs, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m       # of batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory_profiler' is not defined"
     ]
    }
   ],
   "source": [
    "mem = memory_profiler.memory_usage()[0]\n",
    "imgs, targets = next(iter(train_loader))\n",
    "print(f\"       # of batches: {len(train_loader)}\")\n",
    "print(f\"    Image data type: {type(imgs)}\")\n",
    "print(f\"   Image batch size: {imgs.shape}\")  # dimensions are (batch size, image channels, image height, image width)\n",
    "print(f\"  Target batch size: {targets.shape}\")\n",
    "print(f\"       Batch memory: {memory_profiler.memory_usage()[0] - mem:.2f} mb\")  # memory usage after loading batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m VALID_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/bitmoji_rgb/valid/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mVALID_DIR, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[0;32m      4\u001b[0m validloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(valid_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "VALID_DIR = \"data/bitmoji_rgb/valid/\"\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot samples\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m)); plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m); plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Your Name> + Sample Training Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mtranspose(make_grid(sample_batch[\u001b[38;5;241m0\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot samples\n",
    "sample_batch = next(iter(train_loader))\n",
    "plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"<Your Name> + Sample Training Images\")\n",
    "plt.imshow(np.transpose(make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a CNN model\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mbit_train_CNN\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class bit_train_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, (5, 5)), # - 3 input channels (RGB images) - 8 output channels (feature maps)  - Kernel size of 5x5\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)), #- Kernel size of 2x2 (reduces dimensions by half)\n",
    "            nn.Conv2d(8, 4, (3, 3)), # - 8 input channels (from the previous convolutional layer) - 4 output channels (feature maps)  - Kernel size of 3x3\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((3, 3)), #  Max Pooling Layer: Further reduces the spatial dimensions - Kernel size of 3x3\n",
    "            nn.Flatten(),         # Flatten Layer: Converts the multi-dimensional feature maps into a 1D vector\n",
    "            nn.Linear(324, 128), # First Fully Connected (Linear) Layer: Connects the flattened input to 128 neurons  - Input size: 324 (flattened size after convolutions and pooling) - Output size: 128 (hidden layer)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1) # Second Fully Connected (Linear) Layer: Maps the 128 neurons to a single output neuron - Input size: 128 (hidden layer) - Output size: 1 (final prediction, e.g., binary classification)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Train and validate a PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer for updating weights.\n",
    "        trainloader: DataLoader for training data.\n",
    "        validloader: DataLoader for validation data.\n",
    "        epochs: Number of epochs for training.\n",
    "        verbose: If True, print progress for each epoch.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing training loss, validation loss, and validation accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad() # Reset gradients\n",
    "            y_hat = model(X).flatten() # Forward pass: compute the model's prediction\n",
    "            loss = criterion(y_hat, y.type(torch.float32)) # Compute loss between predicted values and actual labels\n",
    "            loss.backward() # Backpropagation: compute gradients\n",
    "            optimizer.step() # Update model parameters using optimizer\n",
    "            train_batch_loss += loss.item() # Accumulate batch loss\n",
    "        train_loss.append(train_batch_loss / len(trainloader)) # Compute average training loss for this epoch\n",
    "        \n",
    "        # Validation\n",
    "        model.eval() # Set the model to evaluation mode (disables dropout/batchnorm layers if any)\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X).flatten() # Forward pass: compute predictions for validation set\n",
    "                y_hat_labels = torch.sigmoid(y_hat) > 0.5 # Apply sigmoid activation and classify outputs (threshold at 0.5)\n",
    "                loss = criterion(y_hat, y.type(torch.float32)) # Compute validation loss\n",
    "                valid_batch_loss += loss.item() # Accumulate validation batch loss\n",
    "                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item() # Compute validation accuracy: compare predicted labels to actual labels\n",
    "        valid_loss.append(valid_batch_loss / len(validloader)) # Compute average validation loss for this epoch\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # Compute average validation accuracy for this epoch\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bit_train_CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define and train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m bit_train_CNN()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the loss function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# nn.BCEWithLogitsLoss() combines a sigmoid activation with binary cross-entropy loss\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# This is typically used for binary classification problems\u001b[39;00m\n\u001b[0;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss() \n",
      "\u001b[1;31mNameError\u001b[0m: name 'bit_train_CNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Define and train model\n",
    "model = bit_train_CNN()\n",
    "\n",
    "# Define the loss function\n",
    "# nn.BCEWithLogitsLoss() combines a sigmoid activation with binary cross-entropy loss\n",
    "# This is typically used for binary classification problems\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "\n",
    "#Define the optimizer\n",
    "# optim.Adam() is an Adam optimizer that adjusts the model's weights using gradients\n",
    "# model.parameters() ensures the optimizer updates all trainable model parameters\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#Train the model using the trainer function\n",
    "results = trainer(model, criterion, optimizer, train_loader, validloader, epochs=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Saving and Loading PyTorch Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [PyTorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) about saving and loading models is fantastic and the process is very easy. It's common PyTorch convention to save models using either a `.pt` or `.pth` file extension. It is recommended that you just save your model learned parameters from `model.state_dict()`:\n",
    "\n",
    "```python\n",
    "# Save model\n",
    "PATH = \"models/my_model.pt\"\n",
    "torch.save(model.state_dict(), PATH)     # save model at PATH\n",
    "# Load model\n",
    "model = MyModelClass()                   # create an instance of the model\n",
    "model.load_state_dict(torch.load(PATH))  # load model from PATH\n",
    "```\n",
    "\n",
    "If you're using the model for inference (not training), make sure to switch it to eval mode: `model.eval()`. There are other options for saving models, in particular, if you want to save a model and continue training it later, you'll want to save other necessary information like the optimizer state, the epoch you're on, etc. This is all documented here in the [PyTorch docs](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your Model 1 - regular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please Save your model 1\n",
    "PATH = \"bitmoji_model.pt\"\n",
    "# PATHExample = \"models/bit_train_CNN_acc85.pt\" \n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "PATH = \"bitmoji_model.pt\"\n",
    "model = bit_train_CNN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is used for two main purposes:\n",
    "1. Make your CNN more robust to scale/rotation/translation in your images\n",
    "2. Increase the size of your training set\n",
    "\n",
    "Let's explore point 1 a bit further. We can see below is a Bitmoji of Tom, does the CNN we loaded above predict this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('img/tom-bitmoji.png')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m IMAGE_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;66;03m# Define the desired image size for the model input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Resize the input image to the specified IMAGE_SIZE and convert it to a tensor\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The unsqueeze(0) adds a batch dimension, as the model expects batched input\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mto_tensor(image\u001b[38;5;241m.\u001b[39mresize((IMAGE_SIZE, IMAGE_SIZE)))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Pass the image tensor through the model, apply sigmoid activation to get probabilities\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the probability into a binary prediction (0 or 1) using a threshold of 0.5\u001b[39;00m\n\u001b[0;32m      8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msigmoid(model(image_tensor)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 64 # Define the desired image size for the model input\n",
    "# Resize the input image to the specified IMAGE_SIZE and convert it to a tensor\n",
    "# The unsqueeze(0) adds a batch dimension, as the model expects batched input\n",
    "image_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0) \n",
    "\n",
    "# Pass the image tensor through the model, apply sigmoid activation to get probabilities\n",
    "# Convert the probability into a binary prediction (0 or 1) using a threshold of 0.5\n",
    "prediction = int(torch.sigmoid(model(image_tensor)) > 0.5)\n",
    "\n",
    "# Print the class corresponding to the prediction using the train_dataset's class list\n",
    "print(f\"Prediction: {train_dataset.classes[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! But what happens if I flip my image. You can still tell it's me, but can my CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.rotate(180)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\n",
    "prediction = int(torch.sigmoid(model(image_tensor)) > 0.5)\n",
    "print(f\"Prediction: {train_dataset.classes[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's problematic. We'd like our CNN to be robust against these kinds of differences. We can expose our CNN to flipped images, so that it can learn to better predict them, with data augmentation. Common image augmentations include:\n",
    "- rotation/flipping\n",
    "- cropping\n",
    "- adding noise\n",
    "- You can view others in the [PyTorch docs](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "We add transforms just like we did previously, using the `transform` argument of `torchvision.datasets.ImageFolder()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5), # Randomly flip the image vertically with a probability of 0.5\n",
    "    transforms.RandomRotation(degrees=20), # Randomly rotate the image by up to ±20 degrees\n",
    "    transforms.Resize(IMAGE_SIZE), # Resize the image to a fixed size defined by IMAGE_SIZE\n",
    "    transforms.ToTensor() # Convert the image to a PyTorch tensor and scale pixel values to [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR,\n",
    "                                                 transform=data_transforms)\n",
    "train_loader_augmented = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=False)\n",
    "sample_batch, target = next(iter(train_loader_augmented))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the augmented images\n",
    "from torchvision.utils import make_grid\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Cole + Sample Training Images\")\n",
    "\n",
    "plt.imshow(np.transpose(make_grid(sample_batch, padding=1, normalize=True),(1,2,0)));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train the augmented model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# GPU available?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# GPU available?\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using: {device}\")\n",
    "# Model\n",
    "model = bit_train_CNN()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bit_train_CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define and train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m bit_train_CNN()\n\u001b[0;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bit_train_CNN' is not defined"
     ]
    }
   ],
   "source": [
    "# Define and train model\n",
    "model = bit_train_CNN()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, train_loader_augmented, validloader, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your Model 2 - Augmented Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "PATH = \"models/bitmoji_cnn_augmented.pt\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = transforms.functional.to_tensor(image.resize((IMAGE_SIZE, IMAGE_SIZE))).unsqueeze(0)\n",
    "prediction = int(torch.sigmoid(model(image_tensor)) > 0.5)\n",
    "print(f\"Prediction: {train_dataset.classes[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract metrics from the results dictionary\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]       \u001b[38;5;66;03m# Training loss per epoch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]       \u001b[38;5;66;03m# Validation loss per epoch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Plot Training and Validation Loss Curve\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metrics from the results dictionary\n",
    "train_loss = results[\"train_loss\"]       # Training loss per epoch\n",
    "valid_loss = results[\"valid_loss\"]       # Validation loss per epoch\n",
    "\n",
    "\n",
    "# Plot Training and Validation Loss Curve\n",
    "plt.figure(figsize=(8, 5))  # Set the figure size\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\", marker='o')  # Training loss\n",
    "plt.plot(range(1, len(valid_loss) + 1), valid_loss, label=\"Validation Loss\", marker='o')  # Validation loss\n",
    "plt.xlabel(\"Epochs\")  # X-axis label\n",
    "plt.ylabel(\"Loss\")  # Y-axis label\n",
    "plt.title(\"Cole M + Training and Validation Loss Curve\")  # Plot title\n",
    "plt.legend()  # Add a legend\n",
    "plt.grid()  # Add grid lines for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Validation Accuracy Curve\n",
    "\n",
    "valid_accuracy = results[\"valid_accuracy\"]  # Validation accuracy per epoch\n",
    "\n",
    "plt.figure(figsize=(8, 5))  # Set the figure size\n",
    "plt.plot(range(1, len(valid_accuracy) + 1), valid_accuracy, label=\"Validation Accuracy\", color='green', marker='o')\n",
    "plt.xlabel(\"Epochs\")  # X-axis label\n",
    "plt.ylabel(\"Accuracy\")  # Y-axis label\n",
    "plt.title(\"Cole M+ Validation Accuracy Curve\")  # Plot title\n",
    "plt.legend()  # Add a legend\n",
    "plt.grid()  # Add grid lines for better readability\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
